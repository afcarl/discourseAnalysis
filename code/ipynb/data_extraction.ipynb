{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction\n",
    "## Wikipedia extraction\n",
    "### narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia as wk\n",
    "p = wk.search('2010s in film')\n",
    "page = wk.page(p[0])\n",
    "s = page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "reg = re.compile('\\n\\n\\n=== 0-9 ===\\n(.*)\\n\\n\\n== See also ==\\n',re.DOTALL)\n",
    "films = reg.findall(s)[0]\n",
    "films = films.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "films = [f for f in films if(not ('===' in f) and len(f)>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "films[0]\n",
    "reg2 = re.compile('\\(.*\\)')\n",
    "titles = []\n",
    "for f in films:\n",
    "    rep = reg2.findall(f)\n",
    "    if(len(rep)>0):\n",
    "        titles.append(f.replace(rep[0],''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg3 = re.compile('\\n\\n\\n== Plot ==\\n(.*?)\\n\\n\\n==',re.DOTALL)\n",
    "plots = []\n",
    "for t in titles:\n",
    "    p = wk.search(t)\n",
    "    print p\n",
    "    try:\n",
    "        page = wk.page(p[0])\n",
    "    except:\n",
    "        continue\n",
    "    cont = page.content\n",
    "    plot = reg3.findall(cont)\n",
    "    if(len(plot)>0):\n",
    "        plots.append((t,plot[0]))\n",
    "        #print plot[0]\n",
    "        #print '\\n\\n\\n'\n",
    "print len(plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(plots)\n",
    "df.to_csv(\"narrative2.csv\",sep='\\t',encoding='utf-8',index=False,header=['title','plot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia as wk\n",
    "p = wk.search('science')\n",
    "page = wk.page(p[0])\n",
    "s = page.content\n",
    "links = page.links\n",
    "print len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg3 = re.compile('(.*?)\\n\\n\\n==',re.DOTALL)\n",
    "articles = []\n",
    "for t in links:\n",
    "    p = wk.search(t)\n",
    "    print p\n",
    "    try:\n",
    "        page = wk.page(p[0])\n",
    "    except:\n",
    "        continue\n",
    "    cont = page.content\n",
    "    article = reg3.findall(cont)\n",
    "    if(len(plot)>0):\n",
    "        articles.append((t,article[0]))\n",
    "\n",
    "print len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_p = [a for a in articles if(len(a[1])>1800)]\n",
    "df = pd.DataFrame(a_p)\n",
    "df.to_csv(\"informative.csv\",sep='\\t',encoding='utf-8',index=False,header=['title','plot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/narrative2.csv',encoding='utf-8',sep='\\t')\n",
    "\n",
    "l =[]\n",
    "for i in range(len(df)):\n",
    "    #print len(df.loc[i]['plot'])\n",
    "    #print '\\n\\n\\n'\n",
    "    if(len(df.loc[i]['plot'])>1800):\n",
    "        l.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/informative.csv',encoding='utf-8',sep='\\t')\n",
    "#df = df.loc[l]\n",
    "write_path='../data/informative/'\n",
    "for i in range(len(df)):\n",
    "    df_tmp = df.loc[i]\n",
    "    filename = unicode(df_tmp['title']).replace(' ','')\n",
    "    filename = filename.replace('/','')\n",
    "    file_tmp = open(write_path+filename+\".txt\", \"wb\")\n",
    "    print i\n",
    "    file_tmp.write(df_tmp[\"plot\"].encode('utf-8'))\n",
    "    file_tmp.close()\n",
    "    #df_tmp[\"plot\"].to_csv(df.loc[i][0],sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web crawling with beautiful soup\n",
    "### argumentative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import time\n",
    "import random\n",
    "\n",
    "r = urllib.urlopen('http://millercenter.org/president/speeches').read()\n",
    "soup = BeautifulSoup(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = soup.findAll('a',attrs={'class':'transcript'})\n",
    "urls = ['http://millercenter.org'+u['href'] for u in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speeches = []\n",
    "for i,u in enumerate(urls):\n",
    "    r = urllib.urlopen(u).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    try:\n",
    "        s = soup.findAll(\"div\",attrs={'id':'transcript'})\n",
    "        speeches.append(s[0].find('p').text)\n",
    "    except:\n",
    "        print'error'\n",
    "        continue            \n",
    "    print i\n",
    "    #time.sleep(random.randint(1, 2) * .931467298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nspeeches = []\n",
    "for i,s in enumerate(speeches):\n",
    "    if(len(s)>1800 and len(s)<10000):\n",
    "        t = urls[i][urls[i].rfind(\"/\")+1:]\n",
    "        nspeeches.append((t,s))\n",
    "        \n",
    "print(len(nspeeches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_path='../data/argumentative/'\n",
    "for i in range(len(nspeeches)):\n",
    "    filename = nspeeches[i][0]\n",
    "    file_tmp = open(write_path+filename+\".txt\", \"wb\")\n",
    "    file_tmp.write(nspeeches[i][1].encode('utf-8'))\n",
    "    file_tmp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
